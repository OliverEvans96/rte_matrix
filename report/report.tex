\documentclass[10pt]{article}

\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{etoolbox}
\usepackage{booktabs}
\usepackage[parfill]{parskip}
\usepackage[numbers]{natbib}
\usepackage{float}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{multicol}
\usepackage{caption}
\usepackage{url}
% \usepackage[T1,T2A]{fontenc}
% \usepackage{lmodern}
% \usepackage[utf8]{inputenc}
% \usepackage[russian,english]{babel}
% 
% \renewcommand{\thesection}{\bfseries\arabic{section}}
% \renewcommand{\thesubsection}{\bfseries\arabic{subsection}}
% \renewcommand{\thesubsubsection}{\bfseries\arabic{subsubsection}}

\newcommand\mgin{0.5in}
\geometry{
    left=\mgin,
    right=\mgin,
    bottom=\mgin,
    top=\mgin
}

% Set path to import figures from
\graphicspath{{../../img/sparsity/}}

% Place converted graphics in current directory
\usepackage[outdir=./]{epstopdf}

% Define multicolumn figure-like environment
% from http://tex.stackexchange.com/questions/12262/multicol-and-figures
\newenvironment{mcfig}
    {\par\medskip\noindent\minipage{\linewidth}}
    {\endminipage\par\medskip}

% Define error function for math mode
\newcommand{\erf}{\mbox{erf}}
% Sign function
\newcommand{\sign}{\mbox{sign}}
% Natural numbers
\newcommand\NN{\mathbb{N}}
% Real numbers
\newcommand\RR{\mathbb{R}}
% Complex numbers
\newcommand\CC{\mathbb{C}}
% Curly B for basis
\newcommand\BB{\mathcal{B}}
% Curly D for diagonal dominance quantity
\newcommand\DD{\mathcal{D}}
\newcommand\QQ{\mathcal{Q}}
% Norm
\newcommand\norm[1]{\left\lVert #1 \right\rVert}
% Uniform Norm
\newcommand\unorm[1]{\left\lVert #1 \right\rVert_\infty}
% Inner Product
\newcommand\ip[1]{\left\langle #1 \right\rangle}
% Absolute value
\newcommand\abs[1]{\left| #1 \right|}
% Complex Conjugate
\newcommand\conj\overline
% Partial derivative
\newcommand\pd[2]{\frac{\partial #1}{\partial #2}}
% Iteration superscript w/ parentheses
\newcommand{\iter}[1]{^{(#1)}}
% Disable paragraph indentation
\setlength{\parindent}{0pt}
% End of proof
\newcommand\qed{\hfill$\blacksquare$\hspace{0.5in}}

% Number this equation
\newcommand\eqnum{\addtocounter{equation}{1}\tag{\theequation}}

% arara: pdflatex
% arara: bibtex
% arara: pdflatex
% arara: pdflatex
\begin{document}

%%fakesection Title
\null

\thispagestyle{empty}
\addtocounter{page}{-1}

\begin{center}
    \begin{sffamily}
    \begin{bfseries}
        \null
        \vfill
        \Huge{Survey of Solution Techniques for Linear Systems from Finite Difference Methods in 2D Numerical Radiative Transfer}

        \vspace{20pt}
		\LARGE{Advanced Numerical Analysis II} \\[.25em]
		\Large{Final Project} \\
        \vspace{20pt}
    \begin{Large}
        Oliver Evans \\
        Fred Weiss \\
        Christopher Parker \\
        Emmanuel Arkoh \\[1em]

        Dr. Malena Espa\~nol \\
    \vspace{20pt}
    \today
    \end{Large}
    \end{bfseries}
    \end{sffamily}
    \vspace{30pt}

    \null
    \vfill
    \vfill
    \null
\end{center}
\pagebreak


% Increase table cell height (not for header)
\renewcommand{\arraystretch}{1.5}

\begin{multicols}{2}

\section{Introduction}
We use monochromatic radiative transfer in order to model the light field in an aqueous environment populated by vegetation.
The vegetation (kelp) is modeled by a spatial probability distribution, which we assume to be given.
The two quantities we seek to compute are \textit{radiance} and \textit{irradiance}.
Radiance is the intensity of light in at a particular point in a particular direction, while irradiance is the total light intensity at a point in space, integrated over all angles.
The Radiative Transfer Equation is an integro-partial differential equation for radiance, which has been used primarily in stellar astrophysics; it's application to marine biology is fairly recent \citep{mobley_radiative_2001}.

We study various methods for solving the system of linear equations resulting from discretizing the Radiative Transfer Equation.
In particular, we consider direct methods, stationary iterative methods, and nonstationary iterative methods.
Numerical experiments are performed using Python's \texttt{scipy.sparse} \citep{jones_scipy:_2001} package for sparse linear algebra.
\texttt{IPython} \citep{perez_ipython:_2007} was used for interactive numerical experimentation.

Among those implemented, the nonstationary \texttt{LGMRES} \citep{baker_technique_2005} algorithm is the only algorithm determined to be suitable for this application without further work.
We discuss limitations and potential improvements, including preconditioning, alternative discretization, and reformulation of the RTE.

\subsection{Radiative Transfer}
Let $n$ be the number of spatial dimensions for the problem (i.e., 2 or 3).
Let $x \in \RR^n$.
Let $\Omega$ be the unit sphere in $\RR^n$.
Let $\omega \in \Omega$ be a unit vector in $\RR^n$.
Let $L(x,\omega)$ denote \textit{radiance} position $x$ in the direction $\omega$.
Let $I(x)$ denote \textit{irradiance} at position $x$.
Let $P_k(x)$ be the probability density of kelp at position $x$.
Let $a_w$ and $a_k$ be the absorption coefficients of water and kelp respectively.
Let $b_w$ and $b_k$ be the scattering coefficients of water and kelp respectively.
Then we define the effective absorption and scattering coefficients as
\begin{align}
	\label{eq:abs}
	a(x) &= P_k(x) a_k + (1-P_k(x)) a_w \\
	\label{eq:sct}
	b(x) &= P_k(x) b_k + (1-P_k(x)) b_w
\end{align}
Then, the Monochromatic Time-Independent Radiative Transfer Equation (RTE) is
\begin{equation}
    \tag{RTE}
    \label{eq:rte}
    \begin{aligned}
        \omega \cdot \nabla_x L(x,\omega) &= -(a(x) + b(x)) L(x,\omega) \\
        &\qquad + b \int_\Omega \beta(\omega \cdot \omega') L(x,\omega')\, d\omega'
    \end{aligned}
\end{equation}

Note that in 2 spatial dimensions, this is a 3-dimensional problem ($x,y,\theta$).
Likewise, in 3 spatial dimensions, it is a 5-dimensional problem ($x,y,z,\theta,\phi$).

In this paper, we consider only the 2-dimensional problem, with the hope that sufficiently robust solution techniques for the 2-dimensional problem will be effective in the solution of the 3-dimensional problem as well.

\subsection{2D Problem}
\label{sec:2d}
We use the downward-pointing coordinate system shown in figure \ref{fig:coords}, measuring $\theta \in [0,2\pi)$ from the positive $x$ axis towards the positive $y$ axis.
Further, we assume that the problem is given on the rescaled spatial domain $[0,1) \times [0,1)$, where $y=0$ is the air-water interface, and $y$ measures depth from the surface.

\begin{figure}[H]
    \centering
    \includegraphics[width=2in]{2d_coords}
    \caption{2D coordinate system}
    \label{fig:coords}
\end{figure}

The 2-dimensional form of \eqref{eq:rte} is given by
\begin{equation}
    \begin{aligned}
        \pd{L}{x} \cos\theta + \pd{L}{y} \sin\theta
        &= -(a+b)L(x,y,\theta) \\
        &+ b\int_0^{2\pi} \beta(\abs{\theta-\theta'})\,d\theta',
    \end{aligned}
    \label{eq:rte2d}
\end{equation}

where $\abs{\theta-\theta'}$ measures the smallest angular difference between $\theta$ and $\theta'$ considering periodicity.

Note that in Cartesian coordinates, there are only spatial, not angular derivatives in the gradient.
In other coordinate systems, this is generally not the case.

\subsection{Boundary Conditions}
We assume that the downwelling light from the surface is known, and is defined to be uniform in space by the Dirichlet boundary condition
\begin{equation}
    L(x,0,\theta) = f(\theta), \quad \mbox{for} \quad \theta \in [0,\pi).
    \label{eq:surf_bc}
\end{equation}

Note that we cannot apply the same idea to upwelling light at the surface, as it cannot be specified from information about the atmospheric light field.
Therefore, we apply the PDE at $y=0$ for $\theta \in [\pi,2\pi)$.

At $y=1$, we assume no upwelling light.
That is,
\begin{equation}
    L(x,0,\theta) = 0, \quad \mbox{for} \quad \theta \in [\pi,2\pi).
    \label{eq:bottom_bc}
\end{equation}

As with the upper $y$-boundary, we apply the PDE for $\theta \in [0,\pi)$ so as not to prohibit downwelling light.

In the horizontal direction, we assume periodic boundary conditions.
Assuming that a single discrete group of plants is being simulated, adjusting the width of the domain effectively modifies the spacing between adjacent groups of plants.

\section{System of Linear Equations}

\subsection{Discretization}
In order to solve \eqref{eq:rte2d} numerically, we discretize the spatial derivatives using 2nd order finite difference approximations, and we discretize the integral according to the Legendre-Gauss quadrature, as described in \citet[Chapter 2]{chandrasekhar_radiative_1960}.
With this in mind, in order to create a spatial-angular grid with $n_x,n_y$, and $n_\theta$ discrete values for $x, y$, and $\theta$ respectively, we use a uniform square spatial discretization with spacing $dx, dy$, and a non-uniform angular discretization according to the roots of the Legendre Polynomial of degree $n_\theta$, denoted $P_{n_\theta}(\theta)$.
When considering square spatial grids, we denote the number of spatial grid points in each dimension by $n_s=n_x=n_y$ and the number of angular grid points by $n_a=n_\theta$.
In each variable, we discard the uppermost grid point, as indicated by the half-open intervals in the previous sections.

Then, we have the grid
\begin{align}
    x_i &= (i-1)\,dx, &\quad i=1,\ldots,n_x \\
    y_j &= (j-1)\,dy, &\quad j=1,\ldots,n_y \\
    \theta_k \,\, \mbox{s.t.}\,\,
    &P_{n_\theta}(\theta_k/\pi-1) = 0, &\quad k=1,\ldots,n_\theta
\end{align}

In the same notational vein, let \\[-1em]
\begin{align}
    L_{ij}^k &= L(x_i,y_j,\theta_k), \\
    \beta_{kl} &= \beta(\abs{\theta_k-\theta_l}), \\
    a_{ij} &= a(x,y) \\
    b_{ij} &= b(x,y)
\end{align}

where $\abs{\cdot}$ is periodic as in \eqref{eq:rte2d}.

For the spatial interior of the domain, we use the 2nd order central difference formula (CD2) to approximate the derivatives, which is
\begin{equation}
    \tag{CD2}
    f'(x) = \frac{f(x+dx)-f(x-dx)}{2dx} + \mathcal{O}(dx^3).
\end{equation}

When applying the PDE on the upper or lower boundary, we use the forward and backward difference (FD2 and BD2) formulas respectively.
Omitting $\mathcal{O}(dx^3)$, we have
\begin{equation}
    \tag{FD2}
    \label{eq:FD2}
    f'(x) = \frac{-3f(x)+2f(x+dx)-f(x+2dx)}{2dx}
\end{equation}
\begin{equation}
    \tag{BD2}
    \label{eq:BD2}
    f'(x) = \frac{3f(x)-2f(x-dx)+f(x-2dx)}{2dx}
\end{equation}

As for the angular integral, we substitute a weighted finite sum of the function evaluated at the angular grid points.
For each $k$, let $a_k$ be the appropriate Legendre-Gauss weight according to \citet[Chapter 2]{chandrasekhar_radiative_1960}.
Then, applying the change of variables theorem to transform from the standard quadrature interval $[0,1]$ to the correct angular interval $[0,2\pi]$, we have
\begin{equation}
    \tag{LG}
    \int_0^{2\pi} f(\theta)\,d\theta \approx \pi\sum_{k=1}^n a_k f(\theta_k)
\end{equation}

\subsection{Difference Equation}
Given the above discrete approximations, the difference equation for \eqref{eq:rte} in the spatial interior of the domain is
\begin{align}
    \label{eq:diffeq}
    \begin{split}
    0 &= \frac{1}{2dx}\left(L_{i+1,j}^k - L_{i-1,j}^k\right) \cos\theta_k
    - \pi b \sum_{\substack{l=1\\ l\neq k}}^{n_\theta} a_l\beta_{kl}L_{ij}^l \\
    &+ \frac{1}{2dy}\left(L_{i,j+1}^k - L_{i,j-1}^k\right) \sin\theta_k
    + (a_{ij} + b_{ij})L_{ij}^k
    \end{split}
\end{align}

Similarly, we discretize using \eqref{eq:FD2} and \eqref{eq:BD2} at the boundaries.
For example, when $j=1$, we have
\begin{align}
    \label{eq:diffeq_bc}
    \begin{split}
    &\quad0 = \frac{1}{2dx}\left(L_{i+1,j}^k - L_{i-1,j}^k\right) \cos\theta_k
    - \pi b \sum_{\substack{l=1\\ l\neq k}}^{n_\theta} a_l\beta_{kl}L_{ij}^l \\
	&+ \frac{1}{2dy}\left(4L_{i,j+1}^k - L_{i,j+2}^k\right) \sin\theta_k
	+ (a_{ij} + b_{ij} -\frac{3}{2dy})L_{ij}^k
    \end{split}
\end{align}

Note that when discretizing the integral, we exclude the $l=k$ term of the sum.
This is because that term corresponds to ``scattering'' straight ahead ($\Delta\theta=0$), which is in fact not scattering at all.
Whether some adjustment to the quadrature is necessary to account for this is unclear.

\subsection{Structure of Linear System}
For each $(i,j,k)$, we have a distinct equation with $4+n_\theta$ variables.
This corresponds to a sparse matrix equation $Ax=b$, each row having $4+n_\theta$ nonzero entries.
Note that $b$ is zero at each row except those which correspond to boundary conditions in $y$.

Each element in $x$ and $b$ correspond to a particular triple $(i,j,k)$, as are each row and column of the coefficient matrix $A$.
In some sense, when we create this linear system, we are unwrapping a 3-dimensional quantity (radiance) into a 1-dimensional vector (the solution, $x$).
Different orders in which the equations are listed can be chosen to generate equivalent systems, so long as the ordering is consistent in $A,b$, and $x$.

The most obvious way to order the equations is to do so via a triple \texttt{for} loop, which has the effect of creating blocks in the matrix corresponding to planes in $(x,y,\theta)$ space.
For example, if the equations are ordered such that the outer \texttt{for} loop is over $x$, and the inner two are over $y$ and $\theta$, then the first $(n_y n_\theta)$ rows of the matrix correspond to the equations for $i=1$.

By switching the order of the \texttt{for} loops, we can generate 6 equivalent matrix systems, each of which can be identified with a permutation of the triple $(0,1,2)$, where 0 corresponds to $x$, 1 corresponds to $y$, and 2 corresponds to $\theta$, and the order of the numbers indicates the order in which the loops were nested, from outer to inner.

The coefficient matrices $A$, generated by each of these choices are shown below for a trivially small system with $n_x=n_y=n_\theta=6$.
Each black square represents a nonzero entry, whereas gray lines are purely for visual aid.

\pagebreak

\subsection{Sparsity Plots}

\newcommand{\spwidth}{2.5in}
\newcommand{\spmgin}{.1in}

\begin{center}
\noindent
\begin{minipage}[t]{.5\textwidth}%
\begin{figure}[H]
    \centering
    \includegraphics[width=\spwidth]{../img/sparsity/int_small_6x6x6_012.eps}
    \caption{Sparsity plot: 6x6x6, ordering 012 }
\end{figure}
\vspace{\spmgin}
\begin{figure}[H]
    \centering
    \includegraphics[width=\spwidth]{../img/sparsity/int_small_6x6x6_021.eps}
    \caption{Sparsity plot: 6x6x6, ordering 021 }
\end{figure}
\vspace{\spmgin}
\begin{figure}[H]
    \centering
    \includegraphics[width=\spwidth]{../img/sparsity/int_small_6x6x6_102.eps}
    \caption{Sparsity plot: 6x6x6, ordering 102 }
\end{figure}
\end{minipage}%
\begin{minipage}[t]{.5\textwidth}%
\begin{figure}[H]
    \centering
    \includegraphics[width=\spwidth]{../img/sparsity/int_small_6x6x6_120.eps}
    \caption{Sparsity plot: 6x6x6, ordering 120 }
\end{figure}
\vspace{\spmgin}
\begin{figure}[H]
    \centering
    \includegraphics[width=\spwidth]{../img/sparsity/int_small_6x6x6_201.eps}
    \caption{Sparsity plot: 6x6x6, ordering 201 }
\end{figure}
\vspace{\spmgin}
\begin{figure}[H]
    \centering
    \includegraphics[width=\spwidth]{../img/sparsity/int_small_6x6x6_210.eps}
    \caption{Sparsity plot: 6x6x6, ordering 210 }
\end{figure}
\end{minipage}%
\end{center}
\pagebreak


\subsection{Physical Setup}
\label{sec:setup}
In the upcoming sections, all numerical experiments are based on the following parameters.

We define the probability density of kelp as 
\begin{equation}
	P_k(x,y) =
	\begin{cases}
		2-y & \mbox{if} \abs{x-.5} \leq 5(1-y)^2\exp(5y-4) \\
		0 & \mbox{otherwise}
	\end{cases}
\end{equation}

We use the normalized volume scattering function defined by
\begin{equation}
	\beta(\theta) = \frac{\exp(-\theta/2)}{2\left(1-\exp(-\theta/2)\right)}
\end{equation}

While this captures the general shape of the VSF of highly scattering water, in practice, we should either model the VSF as a function of parameters, perhaps using Mie Scattering Theory \citep{mobley_volume_2013}, or else interpolate field data from water similar to the water in the region of interest, such as \citet{petzold_volume_1972}.

As a surface boundary condition, we use $L(x,0,\theta) = \sin(\theta)$ for $\theta \in [0,\pi)$, which represents a wide angular distribution of incoming light, centered in the downward direction.

For this set of parameters, the kelp distribution $P_k$ and the highest resolution solution obtained using the methods described below is shown.
\begin{figure}[H]
	\hspace{-.5in}
	\centering
	\includegraphics[width=4in]{img/pk.png}
	\caption{Kelp density $P_k$ for numerical experiments}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=4in]{img/lgmres_best.pdf}
	\caption{Irradiance plot for $n_s=80, n_a=80$ using \texttt{LGMRES}}
\end{figure}

\subsection{Matrix Properties}
There are several properties which a real matrix can have which simplify or accelerate the solution procedure.
Simply from the sparsity patters, we see that none of the above matrices are symmetric.
A symmetric matrix can additionally be positive definite, in which case numerous highly efficient algorithms exist for solving the system, such as the Conjugate Gradient method \citep{nocedal_numerical_1999}.
Another important property is diagonal dominance.

\subsubsection{Diagonal Dominance}
\label{sec:ddom}

A matrix $A=(a_{ij})$ is said to be diagonally dominant if
\begin{equation}
	\abs{a_{ii}} > \sum_{j\neq i}\abs{a_{ij}} \,\, \mbox{for all}\,\, i \\
\end{equation}

For each row $i$ of the matrix, define the quantity
\begin{align}
	\DD_i &= \abs{a_{ii} - \sum_{i\neq j}\abs{a_{ij}}}
\end{align}

Then, $A$ is diagonally dominant if and only if
\begin{equation}
	\min_{1\leq i \leq n} \DD_i > 0.
\end{equation}

Consider the row of the matrix corresponding to an arbitrary interior point, from \eqref{eq:diffeq}.
Then, we have
\begin{align}
	D &= a_{ij} + b_{ij} - \frac{\cos\theta_k}{dx} - \frac{\sin\theta_k}{dy}
	- \pi b_{ij} \sum_{\substack{l=1\\ l\neq k}}^{n_\theta} a_l\beta_{kl}L_{ij}^l
\end{align}

Hence, we wonder whether we can reasonably restrict ourselves to values for $a$ and $b$ which generate diagonal dominance.
We attempt to bound $D$ from below.
First, we must state that 
\begin{align*}
	\sum_{\substack{l=1\\ l\neq k}}^{n_\theta} a_l\beta_{kl}
	&\leq
	\sum_{l=1}^{n_\theta} a_l\beta_{kl}
	\shortintertext{$\because \beta,a_i \geq 0$}
	&\approx \int_0^{2\pi} \beta(\abs{\theta-\theta_k})\, d\theta
	\shortintertext{by Legendre-Gauss quadrature}
	&= \int_{0}^{2\pi} \beta(\abs{\theta})\, d\theta
	\shortintertext{$\because \beta(\abs\cdot)$ is $\pi$-periodic}
	&= 2 \int_{0}^{\pi} \beta(\theta)\, d\theta
	\shortintertext{$\because \beta(\cdot)$ is even}
	&= 2
	\shortintertext{$\because \beta$ is normalized on $[0,\pi]$}
\end{align*}

% As further evidence, we include this numerical plot due to the use of $\approx$ above.
% \begin{figure}[H]
% 	\centering
% 	\hspace{-.5in}
% 	\includegraphics[width=4in]{img/bnd.pdf}
% 	\caption{Numerical evidence that the above sum is bounded above by 2. The lowest line represents $n_\theta=10$, while the uppermost is $n_\theta=160$.}
% \end{figure}

Then, we can bound $D$ from below as follows.

\begin{align}
	D &\geq a_{ij} + b_{ij} - \frac{1}{dx} - \frac{1}{dy}
	- 2\pi b_{ij} \\
	&= a_{ij} - (2\pi - 1)b_{ij} - \frac{1}{dx} - \frac{1}{dy} \\
	&= a_w - (2\pi - 1)b_k - \frac{1}{dx} - \frac{1}{dy} \\
	&\coloneqq \QQ 
\end{align}
since, by \eqref{eq:abs} and \eqref{eq:sct}, $a_{ij} \geq a_w$ and $b_{ij} \leq b_k \, \forall i,j$, assuming that $a_k \geq a_w$ and $b_k \geq b_k$.

If we assume that $dx=dy=ds$, then $\QQ \geq 0$ is equivalent to
\begin{align*}
	a_w - (2\pi - 1)b_k &\geq \frac{2}{ds} = 2n_s.
\end{align*}

For simplicity, we take $a_w=2\pi b_k$. Then, we can be sure that $D \geq 0$ if we take
\begin{equation}
	b_k = 4n_s.
\end{equation}

Then, for example, with $n_s=20$, we take $b_k = 80$, and $a_w = 160\pi$.
For simplicity, we choose $b_w = 80$, and $a_k=2a_w = 320\pi$.
Clearly, these coefficients are orders of magnitude higher than the ones we chose in section \ref{sec:setup}.

We verify, however, that the system \textit{is}, in fact, diagonally dominant under these conditions, and iterative methods are successful.
However, obtaining a numerical solution and plotting the irradiance, we have the following disappointing picture.
\vspace{-1em}
\begin{figure}[H]
	\centering
	\includegraphics[width=4in]{img/ddom.png}
	\caption{Diagonally dominant systems are very dark.}
\end{figure}

As we can see, we are not interested in the small subset of cases for which the coefficient matrix is diagonally dominant, as they require huge absorption and scattering coefficients, far beyond what one would expect from a realistic system.

\section{Direct Methods}
\subsection{Factorizations}
The most straightforward way to solve any linear system is via Gaussian Elimination, or equivalently, by performing an LU factorization.
However, the number of arithmetic operations required for full LU factorization algorithm grows as $\mathcal{O}(n^3)$, making it impractical for most large systems.
However, specific sparse algorithms exist which discard the zero elements, drastically reducing the computational cost.
Similarly, other factorizations which one might generally use to solve a dense system, such as QR, SVD, and Cholesky have analogous sparse algorithms which ignore all zero elements.

Once a factorization has been computed, solving the system is extremely cheap.
This makes factorization particularly appealing in cases where it is necessary to solve numerous systems with the same coefficient matrix $A$, but for different right-hand side vectors $b$.
This occurs in particular for linear time-dependent PDEs.
However, since we are considering the time-independent RTE, that appeal is lost on us.

Another major limitation of factorizations is the large amount of memory they require.
Fundamentally, factorizations require the storage of multiple factors, which need not necessarily preserve the sparsity pattern of the original matrix.
This limitation is somewhat avoidable with out-of-core computations, whereby matrices are stored directly on the hard drive rather than in memory.
Computations directly on disk, however, are anywhere from six times to $10^5$ times slower than computations in memory \citep{jacobs_pathologies_2009}.

\subsection{Software Packages}
Many implementations of direct methods exist for particular circumstances.
\texttt{LAPACK} \citep{anderson_lapack_1999}, the well known free and open source \texttt{FORTRAN} linear algebra library, contains subroutines for banded, triangular, symmetric, tridiagonal, and general matrices.
Of those, the banded solver is potentially applicable to the system we consider.
A variety of more sophisticated free and open source packages for solving sparse systems directly exist, such as \texttt{UMFPACK} \citep{davis_algorithm_2004} and \texttt{MUMPS} \citep{amestoy_mumps:_2001}.
Both of these algorithms in particular are \textit{multi-frontal}, which is to say that they only keep small slices (fronts) of the matrix in memory at any given time, and perform computations on several fronts in parallel, drastically reducing solution time.

\subsection{Numerical comparison}
We briefly compare computational time between standard LU, QR, and SVD factorization algorithms, and then show the dramatic speed increase achieved by sparse algorithms.

\begin{figure}[H]
	\centering
	\includegraphics[width=3.5in]{img/comp.png}
	\caption{Standard factorization algorithms applied to sparse matrices are slow.}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=3.5in]{img/spsvd.png}
	\caption{Sparse-specific algorithms perform much better.}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=3.5in]{img/splu.png}
	\caption{Note that sparse LU is two orders of magnitude faster than sparse QR.}
\end{figure}

\section{Stationary Iterative Methods}
\subsection{Fixed-Point Iteration}
Iterative methods are another way in which to solve linear systems of equations.
Iterative methods work to successively improve upon their approximate solution, stopping when the results are calculated to within a tolerance which can be set as desired.
Iterative methods are a generally good option for solving a linear system $Ax=b$ particularly when $A$ is large.
Since these systems meet that criteria, this was a path that was desirable to explore to see how these results compared.


We follow \citet[Section 1.1]{anderson_analysis_2012} in describing basic Stationary Iterative Methods operate as follows.
Given $Ax = b$, we choose $N$ and $P$ such that 
\begin{equation}
	A = N-P,
	\label{eq:it_split}
\end{equation}
where $N$ is invertible.

Then,
\begin{align*}
    Ax &= b \\
    (N-P)x &= b \\
    Nx &= Px + b
\end{align*}

Extending the idea of fixed point iteration to systems, we take the two instances of $x$ in the above equation to be consecutive iterates, and obtain
\begin{equation}
    x\iter{i+1} = N^{-1}Px\iter{i} + N^{-1}b
    \label{eq:it_np}
\end{equation}

Letting $G=N^{-1}P$ and $C=N^{-1}b$, we have the general form of stationary iterative methods.
\begin{equation}
    x\iter{i+1} = Gx\iter{i} + C
    \label{eq:it_g}
\end{equation}

The choice of $N$ and $P$ in the above equations determine the particular iterative method.
In fact, given that the two are related by \eqref{eq:it_split}, the choice of $N$ uniquely defines a method.

The most common methods are listed here, where $D$, $L$, and $U$ represent the diagonal, strictly lower triangular, and strictly upper triangular pieces of $A$ respectively, $I$ is the identity matrix, $\omega$ and $p$ are free parameters for which the optimal value is problem-dependent.

\begin{figure}[H]
	\centering
	\begin{tabular}{ll}
		\toprule
		Method & $N$ \\
		\midrule
		Jacobi & $D$ \\
		JOR & $\frac{1}{\omega}D$ \\
		Gauss-Seidel & $D + L$ \\
		SOR & $\frac{1}{\omega}D + L$ \\
		Richardson & $-\frac{1}{p}I$ \\
		\bottomrule
	\end{tabular}
	\caption{Basic Linear Stationary Iterative Methods \citep{young_iterative_1971}}
\end{figure}


%There are two types of iterative methods in general and these are Stationary and Krylov subspace.
%The first of these to be explored were the Stationary methods.
%Stationary iterative methods can be expressed in the general form:

%where the $x$ values change from iteration to iteration while $G$ and $C$ remain constant.
%A general advantage of these methods are that they are simple to derive, implement and analyze.
%The disadvantage is that convergence is not guaranteed and for most real-world type problems would not be anticipated.

%Two types of Stationary Iterative methods were explored for these systems, the Jacobi and Gauss-Seidel methods.
%As these methods were described in detail within the scope of the Advanced Numerical Analysis II coursework, these will not be explored in detail.
%Code was developed to implement these algorithms as discussed in the class.

\subsection{Convergence (or lack thereof)}

It is a well-known result \citep{anderson_analysis_2012} that the iterative method defined by \eqref{eq:it_g} converges to the exact solution $x$ if and only if $\rho(G) < 1$, where $\rho(G)$ is the \textit{spectral radius} of $G$, defined as the largest magnitude of an eigenvalue of $G$.
A sufficient but unnecessary condition for convergence of \eqref{eq:it_g} is the diagonal dominance of $A$.

As we observed in section \ref{sec:ddom}, $A$ is in general not diagonally dominant for this problem.
Further, we show that it's eigenvalues need not be bounded in the unit circle.

\subsubsection{Gershgorin Disks}
We can bound the location of eigenvalues in the complex plane using \textit{Gershgorin's Circle Theorem} \citep{gershgorin_uber_1931}, which states that if we define $D(x_0,r)$ as the disk of radius $r$ centered at $x_0$ in the complex plane, then for each eigenvalue $\lambda$ of a matrix $A=(a_{ij})$ with dimension $N$,
\begin{align}
	\lambda &\in \bigcup_{i=1}^N D(a_{ii},R_i),
	\shortintertext{where}
	R_i &= \sum_{\substack{j=1 \\ j \neq i}} a_ij.
\end{align}

For a given iterative method and it's corresponding iteration matrix $G$, we can use this theorem to bound $\rho(G)$.
We show plots for both the standard system considered in this paper, as well as the forcibly diagonally dominant system described in section \ref{sec:ddom} for the Jacobi method.

\begin{figure}[H]
	\centering
	\includegraphics[width=4in]{img/gersh_div.pdf}
	\caption{Gershgorin disks for Jacobi method for standard system}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=4in]{img/gersh_conv.pdf}
	\caption{Gershgorin disks for Jacobi method for diagonally dominant system}
\end{figure}
Note that the original system may have eigenvalues outside of the unit circle, while the modified system must have all eigenvalues within the unit circle.
This demonstrates, as shown previously, that iterative methods will fail in the former case and succeed in the latter case.


% Since the systems would not be guaranteed to converge and there were numerous orderings and resolutions, creating systems that ranged from large to very large, it was helpful to save time to know whether convergence would be expected in advance.
% There were two ways to do this.
% First was to check diagonal dominance.
% Code was developed for this check and resulted in none of the systems having this property.
% However, this was not a necessary criteria so something more specific was desired.
% 
% One of the properties of iterative systems is that it is possible to take the eigenvalues of $G$ (with the system in the general form).
% For any initial guess of $x$, it can be proven that the problem converges if and only if those eigenvalues all lie within the unit circle.
% Code was developed to check this property and ran approximately ten times faster than directly running just a few iterations of the methods directly.
% This was a significant cost savings for larger matrices.
% Unfortunately, the conclusion was that none of the initial matrices would converge through either method under study.

\subsubsection{Preconditioning}

Although the matrix is not naturally suited for stationary iterative methods, it is possible to precondition the matrix in order to induce diagonal dominance.
We consider two such preconditioning strategies.

The first option is the computation of the SVD of $A$, using $U$ and $V^T$ as preconditioners.
However, given the large computational cost of SVD and the lack of readily available efficient sparse algorithms, this was deemed infeasible.

We did, though, explore the method of Jacobi iterations discussed in \citet{yuan_method_2006} to identify and gradually reduce the off-diagonal terms.
This serves to make the system increasingly diagonally dominant until becoming solvable with Stationary Iterative Methods.
 
Our implementation of the Jacobi iteration algorithm successfully preconditioned the matrix corresponding to a $10\times 16$ spatial-angular grid for diagonal dominance, however the time required for preconditioning was on the order of hours, whereas the time required to solve directly or by a nonstationary iterative method was on the order of seconds.
Clearly, this is an inappropriate approach for our problem.
Nonetheless, after preconditioning, both Jacobi and Gauss-Seidel iteration were successfully executed with a tolerance of \texttt{1e-3} and an initial guess of the zero vector.

\begin{figure}[H]
	\centering
	\hspace{-.5in}
	\includegraphics[width=4in]{../img/fred_stat_it/img1.png}
	\caption{The Jacobi method ran in 47 iterations, taking a total computation time of 3.85 seconds.}
\end{figure}

\begin{figure}[H]
	\centering
	\hspace{-.5in}
	\includegraphics[width=4in]{../img/fred_stat_it/img2.png}
	\caption{GS method completed in 8 iterations and taking a total time of 0.75 seconds.}
\end{figure}

In this case, the Gauss-Seidel method was faster and less expensive than Jacobi.
It is also worth noting that fewer Jacobi iterations were necessary in the preconditioning process to allow the Gauss-Seidel method to begin to converge, further making this method more effective for this particular system.

\section{Nonstationary Methods}
Clearly, both direct methods and Stationary Iterative Methods have significant limitations in their ability to solve large sparse systems arising from general multidimensional PDEs.
We therefore consider nonstationary methods.
A nonstationary iterative method is any iterative method which does not have the form \eqref{eq:it_g}.
Nonstationary methods are also called Krylov Subspace methods.
This class of methods that have become increasingly popular in recent decades due to their flexibility and speed \citep{gutknecht_brief_2007}.
Like stationary iterative methods, they do not, in general, require storing the entire matrix at once, as direct methods do.
They do, however, show improved convergence over stationary methods in many cases.

\subsection{Krylov Subspace Methods}

The $k^{\mbox{th}}$ Krylov Subspace generated by a matrix $A \in \mathbb{R}^{n\times n}$ and a vector $v \in \mathbb{R}^n$ is defined as
\begin{equation}
	K_k(A,v) = \mbox{span}\left\{A^iv\right\}_{i=0}^{i=k-1}
\end{equation}

The general approach of a Krylov Subspace method is to successively generate Krylov Subspaces by constructing basis vectors for them, and to find the vector contained in each subspace with the minimum residual $r = \norm{Ax-b}$.
These basis vectors are usually generated with either Arnoldi iteration or bi-Lanczos iteration, both of which apply variants of the Gram-Schmidt orthogonalization process \citep{ghai_comparison_2016}.

The Krylov subspace methods we consider are \texttt{GMRES}, \texttt{LGMRES}, \texttt{BiCG}, \texttt{BiCGSTAB}, \texttt{QMR}, and \texttt{CGS}.
Two other well known Krylov Subspace Methods are \texttt{CG} and \texttt{MINRES}, although they are applicable only to symmetric positive definite systems.

\subsection{Convergence and Preconditioning}
Unlike nonstationary methods for symmetric positive definite matrices, the convergence properties of \texttt{GMRES}, \texttt{BiCG} and other nonsymmetric nonstationary methods are not well understood.
Generally, performance tends to worsen with increasing condition number and improve with increasing eigenvalue clustering.
Preconditioners are very commonly used with Krylov Subspace methods to improve the conditioning of the system in these two regards.
Common preconditioners are Gauss-Seidel, incomplete LU factorization (ILU), and Algebraic Multigrid (AMG), which are fully explored in \citet{ghai_comparison_2016}
We avoid preconditioning in this paper, although based on the \citet{ghai_comparison_2016}, the right conditioner may drastically improve convergence speed, and even induce convergence for a diverging method.
It is therefore an area which we seek to explore in the future.

\section{Numerical Results}

We use Python's \texttt{scipy.linalg.sparse} to apply all of the methods described above to a systems with spatial and angular resolution between $n_s=n_a=10$ and $n_s=n_a=100$.
Calculations were performed using the quad-core, 8GB ram computers in the University of Akron Applied Math Research Lab (AMRL)
Many of these methods failed, and are thus not shown.

For small systems ($60 \times 60$ or smaller), direct methods worked effectively.
In particular, we used \texttt{lu} and \texttt{spsolve} from \texttt{scipy.linalg.solve}.
While \texttt{spsolve} is capable of calling \texttt{UMFPACK}, I was unable to install it on our computers in the AMRL, despite my efforts.
This likely would have shown significant performance improvements over the standard $lu$.
However, for the largest systems, the calculations failed, yielding a memory error.

As stated, no stationary iterative methods were successful.
The nonstationary iterative methods we applied were \texttt{GMRES}, \texttt{LGMRES}, \texttt{BiCG}, \texttt{BiCGSTAB}, \texttt{QMR}, and \texttt{CGS}.
Of them, all but \texttt{LGMRES}, \texttt{GMRES}, and \texttt{CGS} failed explicitly.
\texttt{CGS}, however, essentially returned the zero vector with small perturbations.
\texttt{GMRES}, on the other hand, ran indefinitely.
It may have been eventually successful, though even in the smaller systems, we waited significantly longer than were necessary for the other methods.
\texttt{LGMRES} was the only successful nonstationary method, which was very successful at that.
It was able to handle large matrices very quickly, overcoming the drawbacks of both direct and stationary method.

\begin{figure}[H]
	\centering
	\includegraphics[width=4in]{img/both_dt_log.pdf}
	\caption{Computational time for \texttt{LU} and \texttt{GMRES}}
	\label{fig:conv_time}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=4in]{img/lgmres_time.pdf}
	\caption{Convergence using \texttt{LGMRES} (seconds)}
\end{figure}

We compare convergence times for \texttt{LU} and \texttt{LGMRES} for the smaller matrices as a function of matrix dimension.
For all matrices, we give convergence times as functions of $n_s$ and $n_g$.
Note that computational time seems to increase more rapidly with $n_s$ than $n_a$, which is to be expected since $n_s$ represents both $n_x$ and $n_y$.
Thus, doubling $n_s$ quadruples the size of the system.
Note that matrix size depends on both $n_s$ and $n_a$, which clearly affect CPU time independently, since a single matrix sizes corresponds to multiple times in figure \ref{fig:conv_time}.

\vspace{2em}

\begin{figure}[H]
	\centering
	\includegraphics[width=4in]{img/lgmres_time_3d.png}
	\caption{Irradiance plots for several values of $n_s$ and $n_a$, using \texttt{LGMRES}.}
\end{figure}


\end{multicols}
\pagebreak
\subsection{Convergence Study}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/lgmres_conv.pdf}
	\caption{Irradiance plots for several spatial and angular resolutions.}
\end{figure}
\begin{multicols}{2}

We show irradiance plots for several systems obtained using \texttt{LGMRES}.
Recall that irradiance is defined as a function of space only, not angle, hence we can display it as a two-dimensional image for our two-dimensional problem.
Precisely,
\begin{equation}
	I(x,y) = \int_0^{2\pi} L(x,y,\theta)\, d\theta
\end{equation}

Visually, the $100 \times 100$ plot seems sufficiently resolved, although of course, the definition of ``sufficiently'' varies from case to case.
Further, we observe that in the cases with lower angular resolution (e.g. $na=20,ns=100$), the paths along which light can travel are clearly visible as discontinuities occurring along straight lines.
This leads to the conclusion that angular resolution is more important than spatial resolution for obtaining accurate solutions.

We also compare irradiance values among all grid resolutions as follows.
We consider the set of spatial grid points in the lowest resolution trial, namely $20 \times 20$ here, and compare irradiances from all spatial and angular resolutions at those particular locations.
This is possible since the number of spatial grid points is in each case a multiple of the number in the coarsest case.
We treat the highest resolution, here $100 x 100$, as the ``true'' solution, and differences respectively.
We report the average of these errors over space for each resolution.
Comparing maximum values results in a similar image.
We also display in figure \ref{fig:diff} that \texttt{LU} and \texttt{LGMRES} produce very similar values, leading us to trust solutions from \texttt{LGMRES} in higher resolutions where \texttt{LU} is not feasible.


\end{multicols}
\begin{figure}[H]
	\centering
	\includegraphics[width=7in]{img/diffgrid.pdf}
	\caption{Difference between \texttt{LU} and \texttt{LGMRES}. All values are less than \texttt{5e-5}, indicating nearly identical results.}
	\label{fig:diff}
\end{figure}

\begin{multicols}{2}
\begin{figure}[H]
	\centering
	\hspace{-.5in}
	\includegraphics[width=3.5in]{img/avgerr.pdf}
	\caption{Average error per grid point v.s. $100 \times 100$.}
\end{figure}
\begin{figure}[H]
	\centering
	\hspace{-.5in}
	\includegraphics[width=3.5in]{img/avgerr_3d.png}
	\caption{Average error per grid point v.s. $100 \times 100$.}
\end{figure}

\section{Conclusions and Further Work}

We explore a variety of numerical solution techniques for the linear system generated by discretizing the Monochromatic Time-Independent Radiative Transfer Equation using finite differences.
We find that in general, the system fails to be diagonally dominant, and thus stationary iterative methods fail to converge.
While direct methods are effective for small systems, they are both too slow and require too much memory for larger problems.
Hence, Krylov subspace methods are found to be the only feasible solution.

In particular, \texttt{LGMRES} was consistently convergent for the set of parameters used in the numerical experiments in this paper.
It is not certain whether the method would continue to converge upon varying parameters.
Further, the numerical experiments were carried out using the 210 ordering for the coefficient matrix.
It may be the case that different orderings have differing convergence properties.

In future work, we will explore different kelp distributions, which may or may not alter the convergence of each method.
We also intend to experiment with preconditioning to improve convergence.
In particular \texttt{AMG} seems appropriate, as it is designed with block structure in mind.

Once we are confident in the accuracy of our numerical solution technique, we will begin developing a constitutive model for irradiance as a function the spatial kelp distribution.
This will serve to improve our understanding of the role of light and self-shading in kelp aquaculture.
\vspace{-1em}

%\nocite{*}
\bibliographystyle{abbrvnat}
\bibliography{rte_matrix_report}
\end{multicols}
\end{document}


